# ğŸ“Š Bivariate Analysis â€“ Data Preprocessing & Exploratory Data Analysis

Welcome to the **Bivariate Analysis** part of the *Data Preprocessing and Exploratory Data Analysis (EDA)* project!  
This notebook explores how **two variables relate to each other**, which is one of the most important steps in understanding your data before building any machine learning model.

Weâ€™ll use built-in datasets from **scikit-learn** to learn key concepts with real examples.

---

## ğŸ“˜ What Is Bivariate Analysis?

**Bivariate analysis** means studying **two variables at the same time** to see if they are related and how strong that relationship is.

It helps you answer questions like:

- ğŸ“ˆ Do two numerical variables change together?  
- ğŸ“Š Are two categories connected or independent?  
- ğŸ§ª Are some features too closely related, causing multicollinearity?

---

## ğŸ“š Topics Covered

| ğŸ” Scenario | ğŸ“Š Technique | ğŸ§  Purpose |
|------------|--------------|------------|
| Continuous â†” Continuous | Correlation Matrix | Measure linear relationships |
| Continuous â†” Continuous | VIF (Variance Inflation Factor) | Detect multicollinearity |
| Categorical â†” Categorical | Chi-Square Test | Test if variables are dependent |

---

## ğŸ“‚ Datasets Used

We use two well-known datasets available directly in scikit-learn:

- ğŸŒ¸ **Iris dataset** â€“ used for classification tasks.  
- ğŸ  **California Housing dataset** â€“ used for regression and correlation studies.

---

## ğŸ› ï¸ How to Use

1. Open the notebook:  
   `Bivariate Analysis/bivariate analysis.ipynb`
2. Run all cells step by step.
3. Watch how relationships between variables are discovered through code, visuals, and statistical tests.

---

## ğŸ“Š Continuous vs Continuous

We start by studying relationships between continuous (numerical) variables.  
A **correlation matrix heatmap** shows how strongly each pair of features is related:

```python
sns.heatmap(california_pd.corr(method="pearson"), annot=True, cmap="Blues")
```

ğŸ“Œ **How to read it:**  
- Values close to **1** â†’ strong positive correlation  
- Values close to **-1** â†’ strong negative correlation  
- Values near **0** â†’ little or no linear relationship

---

### ğŸ§  Variance Inflation Factor (VIF)

**VIF** helps detect **multicollinearity** â€” when two or more features are too strongly related, which can harm regression models.

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif
```

âœ… **Tip:** A VIF above 5 or 10 suggests the feature might be redundant.

---

## ğŸ“Š Categorical vs Categorical

For categorical variables, we use the **Chi-Square Test** to see if two variables are related or independent.

```python
from scipy.stats import chi2_contingency

chi2, p, dof, ex = chi2_contingency(pd.crosstab(df['col1'], df['col2']))
```

ğŸ“Œ **How to interpret:**  
- **p < 0.05** â†’ significant relationship (variables are dependent)  
- **p â‰¥ 0.05** â†’ no significant relationship (variables are independent)

---

## ğŸ“ˆ Visuals Youâ€™ll See

âœ¨ Correlation heatmaps â€“ to visualize numeric relationships  
ğŸ“‰ Scatterplots â€“ to observe variable patterns  
ğŸ“Š Chi-Square summary tables â€“ to check categorical associations  

All of these help you **see relationships clearly** rather than just reading numbers.

---

## ğŸ§  Key Takeaways

- **Correlation** helps identify how continuous features are related.  
- **VIF** detects multicollinearity and improves model quality.  
- **Chi-Square Test** reveals whether categorical variables depend on each other.  

Together, these tools give you a deeper understanding of your dataset and guide better feature selection before modeling.

---

